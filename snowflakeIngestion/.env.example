# =============================================================================
# Snowflake Data Ingestion - Environment Variables
# =============================================================================
# Copy this file to .env and fill in your actual values:
#   cp .env.example .env
#
# Then load variables:
#   export $(cat .env | xargs)
#   OR
#   source .env (if using a tool that supports it)
# =============================================================================

# -----------------------------------------------------------------------------
# Snowflake Connection
# -----------------------------------------------------------------------------
# Account identifier (e.g., UKUYSXQ-HJ26222 or account_locator.region.cloud)
SNOWFLAKE_ACCOUNT=your_account_identifier

# Snowflake username
SNOWFLAKE_USER=your_username

# Snowflake password
SNOWFLAKE_PASSWORD=your_password

# Warehouse name (default: COMPUTE_WH)
SNOWFLAKE_WAREHOUSE=COMPUTE_WH

# Database name (default: SUPPLYCHAINDB)
SNOWFLAKE_DATABASE=SUPPLYCHAINDB

# Schema name (default: RAWDATA)
SNOWFLAKE_SCHEMA=RAWDATA

# -----------------------------------------------------------------------------
# Snowflake S3 Integration
# -----------------------------------------------------------------------------
# Storage integration name created in Snowflake
# Get value: SHOW INTEGRATIONS; or check Terraform outputs
SNOWFLAKE_STORAGE_INTEGRATION=supplyChainS3Integration

# -----------------------------------------------------------------------------
# AWS S3 Configuration
# -----------------------------------------------------------------------------
# S3 bucket name containing processed files
S3_BUCKET_NAME=dataco-supply-chain-analytics

# S3 prefix path for processed files (include trailing slash)
S3_PROCESSED_PREFIX=processed/
