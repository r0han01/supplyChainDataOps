#!/bin/bash

set -e

echo "üöÄ DataCo Supply Chain Analytics - Setup"
echo "========================================"

# Check if .env file exists
if [ ! -f .env ]; then
    echo "‚ö†Ô∏è  .env file not found. Creating from .env.example..."
    cp .env.example .env
    echo "‚úÖ Created .env file. Please update it with your credentials."
    echo ""
    echo "Required:"
    echo "  - KAGGLE_API_TOKEN (get from https://www.kaggle.com/settings/account)"
    echo "  - AWS credentials (configure via 'aws configure')"
    echo ""
    exit 1
fi

# Load environment variables
source .env

# Check Python version
if ! command -v python3 &> /dev/null; then
    echo "‚ùå Python 3 is required but not installed."
    exit 1
fi

echo "‚úì Python 3 found"

# Create virtual environment if it doesn't exist
if [ ! -d "venv" ]; then
    echo "üì¶ Creating virtual environment..."
    python3 -m venv venv
fi

# Activate virtual environment
source venv/bin/activate

# Install dependencies
echo "üì¶ Installing dependencies..."
pip install -q --upgrade pip
pip install -q -r requirements.txt

echo "‚úÖ Dependencies installed"

# Create necessary directories
mkdir -p localData
echo "‚úÖ Created localData directory for dataset downloads"

# Verify AWS credentials
if ! aws sts get-caller-identity &> /dev/null; then
    echo "‚ö†Ô∏è  AWS credentials not configured. Run 'aws configure' first."
    exit 1
fi

echo "‚úÖ AWS credentials verified"

# Run data fetcher
echo ""
echo "üì• Fetching data from Kaggle..."
python dataFetcher/dataFetcher.py

echo ""
echo "‚úÖ Setup complete!"
echo ""
echo "Next steps:"
echo "  1. Open Alteryx Designer Cloud"
echo "  2. Run workflow: ordersDataPreparation"
echo "  3. Run workflow: clickstreamDataPreparation"
echo "  4. Verify processed files in S3"
